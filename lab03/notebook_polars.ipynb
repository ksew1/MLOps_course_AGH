{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15710735-72e1-48cc-b404-bd7e7d09b0d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Polars introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de66090e-1982-4ac9-a5d6-db28ef65d8c2",
   "metadata": {},
   "source": [
    "This notebook is loosely based on:\n",
    "- [\"A Practical Introduction to Polars for Python Data Processing\" P. Darugar](https://www.parand.com/a-practical-introduction-to-polars.html)\n",
    "- [\"Modern Polars\" K. Heavey](https://kevinheavey.github.io/modern-polars/)\n",
    "- [Polars user guide](https://pola-rs.github.io/polars-book/user-guide/)\n",
    "- [Polars blog - A bird's eye view of Polars](https://pola.rs/posts/polars_birds_eye_view/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca15f7e-ddd4-4347-95ba-6ef62a1cef7b",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc1b13-f9a9-4158-9727-8ef908f9b2a1",
   "metadata": {},
   "source": [
    "We will use [New York City yellow taxi trips](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) from 2024. This is a typical raw input to ML pipelines, with individual events as rows. We will download full 2024 data. Due to its size, it's already distributed as Parquet files. Feature descriptions are available [as a data dictionary document](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cd0f3-f57c-4779-9f89-3fd116d3f33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "\n",
    "for month in range(1, 13):\n",
    "    if not os.path.exists(f\"{DATA_DIR}/{month}.parquet\"):\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"wget\",\n",
    "                f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02}.parquet\",\n",
    "                \"-O\",\n",
    "                f\"{DATA_DIR}/{month}.parquet\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "total_size = sum(\n",
    "    os.path.getsize(f\"{DATA_DIR}/{month}.parquet\") for month in range(1, 13)\n",
    ")  # bytes\n",
    "total_size_mb = total_size // (1024 * 1024)\n",
    "print(f\"Total dataset size: {total_size_mb} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cf618-247b-4559-b900-8d046fa3ea24",
   "metadata": {},
   "source": [
    "For later queries, we will also download mapping for location IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28473247-a398-478a-ab7f-e668cda55276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{DATA_DIR}/taxi_zone_lookup.csv\"):\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"wget\",\n",
    "            \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\",\n",
    "            \"-O\",\n",
    "            f\"{DATA_DIR}/taxi_zone_lookup.csv\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c68e8a-48c9-410f-a233-71c902910d69",
   "metadata": {},
   "source": [
    "## DataFrames, selecting and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6e0ae-712f-4627-8a41-c171633aa323",
   "metadata": {},
   "source": [
    "Polars works generally very similarly to Pandas. We will use mostly eager evaluation here. If you run into memory problems, use only later month, e.g. 6-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a2447-2b9b-4a53-b5fd-93fe8d073ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_parquet(f\"{DATA_DIR}/1.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d82c5-256b-4e86-a16d-37b173c5be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3330c-0b7b-43fa-bcbd-e2fbb6ffcbfe",
   "metadata": {},
   "source": [
    "Checking columns, data types, and shape is also similar to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6888f-7806-4e4d-bce8-aacadb05e53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Python list of strings\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746407a6-20be-428c-8725-3a67e6d9ba60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Python list of data types from pd.datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1393e-812a-49f6-8e6d-a1d9924f4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple of integers\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30ba94-91e4-4366-afe2-61268d7b2075",
   "metadata": {},
   "source": [
    "A few basics:\n",
    "- **selecting** columns is done with `.select()`\n",
    "- **filtering** rows is done with `.filter()`\n",
    "- **columns** are used either as strings for simple uses, or as objects created with `pl.col(col_name)` for more complex transformations\n",
    "- **expressions** are lazily evaluated objects (always, even in eager mode), representing data transformations\n",
    "- **methods** are typically run on expressions, e.g. `.is_null()`\n",
    "\n",
    "Selecting and filtering operations take columns or expressions. A single column is also an expression. Let's see a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b2bc6-11ed-40a7-be12-f28e9be82312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"passenger_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3712c-a35e-40b4-9f44-743912fce177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(pl.col(\"passenger_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ef2e9-fe11-4e64-84f8-e068645054fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.col(\"passenger_count\").is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51406a91-c296-4a46-b646-94bbf8871e9f",
   "metadata": {},
   "source": [
    "Filters can be combined with `&` (AND) and `|` (OR), or negated with `~`, like in Pandas. For AND, you can also pass filters as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1738167-17d3-41e9-8ab6-bb9753ecd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.col(\"passenger_count\").is_not_null(), pl.col(\"tip_amount\") < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f329b4-9ef0-406b-a56d-308bac33d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative syntax with &\n",
    "df.filter((pl.col(\"passenger_count\").is_not_null()) & (pl.col(\"tip_amount\") < 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf9466-17de-4bda-bfb9-314b002a2ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.filter((pl.col(\"passenger_count\").is_not_null()) | (pl.col(\"tip_amount\") < 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f03e20-20e8-49df-89f5-1db13c4cd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly for selecting many columns, you can also use strings directly\n",
    "df.select([\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f74fc-b413-4f51-91ec-aebf9acf9603",
   "metadata": {},
   "source": [
    "There is also a shortcut syntax for selecting columns with `[]`, like in Pandas. It's quite limited though, you can also read the data this way, and not e.g. create new columns or assign values. However, it's useful for simple analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac5ad9-db7d-4b80-8676-032db3a65e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tip_amount\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b4ba6-48a3-4f6c-bba6-d74ff52f11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .mean(), .sum() etc. ignore NULL values by default, like in Pandas\n",
    "df[\"passenger_count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdb38b-ba4c-4d76-9fda-336526b1d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can combine both syntax styles\n",
    "df.filter(pl.col(\"Airport_fee\") == 0)[\"total_amount\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f1a8f-ecc3-4028-9187-b344e57a07e1",
   "metadata": {},
   "source": [
    "To create new columns or overwrite values of existing ones, use `.with_columns()`. You can rename them either by using a keyword argument, or by using `.alias()`. You can also rename columns with `.rename()`. If you don't need any old column, just create a value and use it further, you can just use `.select()` and create column inside.\n",
    "\n",
    "For chaining many operations, it's very useful to wrap the entire pipelines in parentheses `()`. This is a proper multiline Python syntax for making many method calls one after another, and a great pattern overall. It avoids making ugly backslash `\\` after every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b043bb-bbd3-42a6-a895-320570b37632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set strict=False so you can run this cell twice without errors\n",
    "df = df.rename({\"Airport_fee\": \"airport_fee\"}, strict=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa8480-b725-4bb0-a437-82fc2b2392b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the average trip time?\n",
    "(\n",
    "    df.with_columns(\n",
    "        (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).alias(\n",
    "            \"trip_time\"\n",
    "        )\n",
    "    )\n",
    "    .select(\"trip_time\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bd6e6-dc71-4fdb-bf26-eb98c8efd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way of aliasing columns are keyword arguments\n",
    "(\n",
    "    df.with_columns(\n",
    "        trip_time=(pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\"))\n",
    "    )\n",
    "    .select(\"trip_time\")\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c2500-de8d-424f-8d53-a4050493c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can build any pipelines, data filtering etc. this way\n",
    "(\n",
    "    df.filter(pl.col(\"passenger_count\").is_not_null(), pl.col(\"tip_amount\") < 1.0)\n",
    "    .select(\n",
    "        (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).alias(\n",
    "            \"trip_time\"\n",
    "        )\n",
    "    )\n",
    "    .median()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7216b78-9afa-4512-ad40-a5d46f3a23ce",
   "metadata": {},
   "source": [
    "Since Polars expressions are objects, they can be created beforehand. They are executed lazily, only when you call appropriate methods on the actual DataFrame. This is very useful for building more complex pipelines, as it helps avoid code repetition with common operations and increases readability with variable names. You can also write regular Python functions, objects, and tests working on Polars expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d97fee-4ac7-4fbd-b386-c637f0d1a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same pipeline as above, but with pre-built expressions\n",
    "trip_time = (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\")).alias(\n",
    "    \"trip_time\"\n",
    ")\n",
    "non_tipping_passengers = pl.col(\"passenger_count\").is_not_null() & (\n",
    "    pl.col(\"tip_amount\") < 1.0\n",
    ")\n",
    "\n",
    "(df.filter(non_tipping_passengers).select(trip_time).median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324f625-6bec-4b14-a4c0-a2d62f699841",
   "metadata": {},
   "source": [
    "Results of Polars data processing can be easily transformed to Pandas DataFrame for integration with frameworks that don't support Polars directly. This uses `.to_pandas()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb729a8-d395-4c1b-b829-a687a069b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_passengers = (\n",
    "    df.filter(pl.col(\"passenger_count\").is_not_null())\n",
    "    .select(\"passenger_count\")\n",
    "    .to_pandas()\n",
    ")\n",
    "df_passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472cfb6-1946-4bd7-a105-0b745304cc69",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. How many passengers paid by card, and how many by cash? Check [the data dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) and `payment_type` meaning.\n",
    "2. What percentage of passengers paying by card gave any tip?\n",
    "3. What percentage of passengers had to pay any additional charge (`extra`, `tolls_amount`, `congestion_surcharge`, or `airport_fee`)?\n",
    "4. Plot a histogram of trip times with a single passenger. Exclude any trips over 120 minutes. Use [temporal .dt attributes](https://docs.pola.rs/api/python/stable/reference/expressions/temporal.html).\n",
    "5. What is the Pearson correlation between trip time (in minutes) and trip length?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58672140-706b-488b-b18b-583e8a4ca8e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceeb567d-7586-4acd-9f04-fbd38a5ecf83",
   "metadata": {},
   "source": [
    "## Joining and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dee496d-268a-452d-93a4-922658118385",
   "metadata": {},
   "source": [
    "Polars also supports other typical SQL operations for analytics:\n",
    "- joining DataFrames with `.join()` method\n",
    "- GROUP BY with `.group_by()`\n",
    "- windowing functions for time aggregations with `.group_by_dynamic()`\n",
    "- sorting data with `.sort()`\n",
    "\n",
    "Making an alias to explicitly name a column after grouping is very useful.\n",
    "\n",
    "As an example of grouping, we will calculate the average cost of trips starting in each location (PU - pickup location, DO - dropoff location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a942a5-f001-438a-869d-23accb649113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pu_location_means = df.group_by([\"PULocationID\"]).agg(\n",
    "    pl.col(\"total_amount\").mean().alias(\"average_cost\")\n",
    ")\n",
    "df_pu_location_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f52dd-09a9-449b-868a-d1114ca69a69",
   "metadata": {},
   "source": [
    "Those IDs aren't very informative. However, we can join them with information from `taxi_zone_lookup.csv`, to get a clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0a8ff-e888-49a6-839b-4dcb9f981c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_zones = pl.read_csv(\"data/taxi_zone_lookup.csv\")\n",
    "df_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450b3f4-3d78-4b51-8865-4cfc8373e77e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df_pu_location_means.join(\n",
    "        df_taxi_zones, left_on=\"PULocationID\", right_on=\"LocationID\"\n",
    "    )\n",
    "    .select([\"Borough\", \"Zone\", \"average_cost\"])\n",
    "    .sort(\"average_cost\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e87f41-b666-4465-bd71-123a5f5993a0",
   "metadata": {},
   "source": [
    "Grouping this way is perfect for analyzing different data segments, such as regions or customer types. However, for analyzing groups changing in time, SQL and DataFrame framework use a concept of windows and window functions. They are basically groups, but dynamic in time. Those calculations are very common in analytics, but also very computationally intensive.\n",
    "\n",
    "As an example, we will compute the average daily travel time in different days. Note that in Polars, window functions need a sorted input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a38cb-4582-4d7d-aa8f-7ed1f8557183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_daily_avg_trip_times = (\n",
    "    df.with_columns(\n",
    "        (pl.col(\"tpep_dropoff_datetime\") - pl.col(\"tpep_pickup_datetime\"))\n",
    "        .dt.total_minutes()\n",
    "        .alias(\"trip_time\")\n",
    "    )\n",
    "    # we have some errors in the data, we'll clean them in the next section\n",
    "    .filter(pl.col(\"trip_time\") < 120)\n",
    "    .filter(pl.col(\"tpep_dropoff_datetime\").dt.year() == 2024)\n",
    "    .sort(\"tpep_dropoff_datetime\")\n",
    "    .group_by_dynamic(\"tpep_dropoff_datetime\", every=\"1d\")\n",
    "    .agg(pl.col(\"trip_time\").mean().alias(\"avg_trip_time\"))\n",
    "    .to_pandas()\n",
    "    .plot.line(x=\"tpep_dropoff_datetime\", y=\"avg_trip_time\")\n",
    ")\n",
    "df_daily_avg_trip_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444794b-44ca-47ed-bbb4-2cef2166b3dc",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. What is the mean distance of trips with different number of passengers? Plot them on a bar plot. Exclude null values.\n",
    "2. What is the median total trip cost (`total_amount`) during the day, and during the night? Assume that day hours are 6:30-19:30. [pl.time](https://docs.pola.rs/api/python/dev/reference/expressions/api/polars.time.html) may be useful.\n",
    "3. What are the top 5 pairs of boroughs with the highest average trip time?\n",
    "4. Calculate average costs of trip per minute, if starts and ends in the same borough and zone. What are the names of 5 boroughs and zones with the highest cost? Ignore trips under 1 minute.\n",
    "5. What are daily total number of trips? Plot them on a line plot. Include only samples from 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d31bbd-b972-443d-82d5-d16bb0eedc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb239c61-d23c-4c5a-a521-91111bdc5fe1",
   "metadata": {},
   "source": [
    "## Data cleaning and prepreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9867f-1372-40d2-828e-fc7673cd8c0f",
   "metadata": {},
   "source": [
    "Many operations of data cleaning, transforming, merging, deduplication etc. are done on the level of DataFrame processing. Others, like scaling or filling missing values (e.g. with mean or median) in ML, are more frequently implemented as scikit-learn pipelines. It depends on a use case, but many times we need to preprocess data as an integral part of creating the resulting dataset.\n",
    "\n",
    "Important Polars functions related to data cleaning\n",
    "- `.describe()` - summarize column statistics\n",
    "- `.is_null()`, `.is_not_null()` - checking and filtering missing values\n",
    "- `.drop_nulls()` - removing rows with missing values\n",
    "- `.drop()` - removing columns\n",
    "- `.fill_null()` - fills missing values\n",
    "- `.cast()` - change column data type\n",
    "- `.to_dummies()` - use one-hot encoding with dummy variables for categorical column\n",
    "- `.unique()` - drops duplicate rows by a given column\n",
    "\n",
    "As an example, let's see the date range in out data. It should be only January 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430ac2f-8a91-453d-bc7c-07a9bca0d9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.select(\n",
    "        pl.col(\"tpep_pickup_datetime\").min().alias(\"pickup_time_min\"),\n",
    "        pl.col(\"tpep_pickup_datetime\").max().alias(\"pickup_time_max\"),\n",
    "        pl.col(\"tpep_dropoff_datetime\").min().alias(\"dropoff_time_min\"),\n",
    "        pl.col(\"tpep_dropoff_datetime\").max().alias(\"dropoff_time_max\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a7e00-dec0-44e9-a685-b9dc865a2585",
   "metadata": {},
   "source": [
    "Smallest times are from 2002, that's definitely wrong! End times slightly in February are more acceptable.\n",
    "\n",
    "Let's also check passenger counts. NYC taxis take at most 6 passengers, larger numbers are group rides, provided in another file. This number is written manually by the taxi driver, so we also expect many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a86a6-5921-43e7-868f-ae3715e740b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_6_passengers_count = len(df.filter(pl.col(\"passenger_count\") > 6))\n",
    "null_passengers_count = len(df.filter(pl.col(\"passenger_count\").is_null()))\n",
    "\n",
    "over_6_passengers_perc = over_6_passengers_count / len(df)\n",
    "null_passengers_perc = null_passengers_count / len(df)\n",
    "\n",
    "print(f\"Over 6 passengers: {over_6_passengers_count} ({over_6_passengers_perc:.3%})\")\n",
    "print(f\"Null passengers: {null_passengers_count} ({null_passengers_perc:.3%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e401b-27d6-4972-895d-c187bdf0b572",
   "metadata": {},
   "source": [
    "To check null values in all columns, `.describe()` works really well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90845d01-125b-4b55-a139-9522f082ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11e9d4-548f-4fc9-9585-8acec35bab96",
   "metadata": {},
   "source": [
    "Note that there is exactly the same number of NULL values in all columns. This can also indicate more systematic problem and deeper real-life investigation. Here, we should probably remove that data.\n",
    "\n",
    "If we wanted to use this data in machine learning pipelines, e.g. for forecasting number of taxi rides, the borough information could be very useful. However, it's a categorical variable, and very few algorithms support them natively (notably XGBoost, LightGBM, CatBoost). Encoding with dummy variables would be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ddbc8-4d73-4993-9d7a-2e373d18016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3054c8-a6e7-4c60-954f-1f768ef6f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_taxi_zones.select([\"LocationID\", \"Borough\"]).to_dummies(\"Borough\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c401e-95bc-42bb-a62d-c0bb98c11b9f",
   "metadata": {},
   "source": [
    "For selecting columns after dummy encoding, e.g. for grouping aggregations, you can use [regular expressions](https://regex101.com/) to [select columns](https://docs.pola.rs/api/python/version/0.18/reference/expressions/api/polars.col.html) in `pl.col()`. Regular expressions need to start with `^` and end with `$`. You can also pass such general expressions as aggregations directly.\n",
    "\n",
    "As an example, let's compute a total number of rides from different borough for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81232b13-7206-4b5d-8d1b-2fdcf46820f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.join(df_taxi_zones, left_on=\"PULocationID\", right_on=\"LocationID\")\n",
    "    .with_columns(pl.col(\"tpep_pickup_datetime\").dt.date().alias(\"date\"))\n",
    "    .select([\"date\", \"passenger_count\", \"Borough\"])\n",
    "    .to_dummies(\"Borough\")\n",
    "    .group_by(\"date\")\n",
    "    # select all columns with name starting with \"Borough_\" and anything further\n",
    "    .agg(pl.col(\"^Borough_.*$\").sum())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d6ee5-f6ec-4e9c-8a00-854152d06b9d",
   "metadata": {},
   "source": [
    "Lastly, it's often useful to reduce the number of bits. By default, Polars is quite conservative and uses quite large integers. We can use a much smaller number in many cases, e.g. for categorical variables, small counts, or identifiers. This reduces memory usage and also speeds up calculations on modern processors, which can pack more data for vector operations this way.\n",
    "\n",
    "Here, two functions are useful:\n",
    "- `.schema()` to get column names and types\n",
    "- `.describe()` to get minimal and maximal values of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95aeee-0281-412d-a592-3393bb90d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7647c3-969e-4318-8f81-ec75e76ca470",
   "metadata": {},
   "source": [
    "For example, we see that `passenger_count` could be a small unsigned integer, as it's surely under 255. See [documentation](https://docs.pola.rs/api/python/stable/reference/datatypes.html) for a list of data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecabfc0-56a8-47ca-aaca-7210f69ed7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "df = df.with_columns(pl.col(\"passenger_count\").cast(pl.UInt8))"
  },
  {
   "cell_type": "markdown",
   "id": "63fee5c1-4c9d-414b-9f15-b1059c4f44e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:28:46.618583Z",
     "iopub.status.busy": "2025-03-09T19:28:46.618418Z",
     "iopub.status.idle": "2025-03-09T19:28:46.624733Z",
     "shell.execute_reply": "2025-03-09T19:28:46.624340Z",
     "shell.execute_reply.started": "2025-03-09T19:28:46.618574Z"
    }
   },
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Keep only rows from 2024.\n",
    "2. Check how many rows have zero passengers. Keep rows that have between 1 and 6 passengers (inclusive).\n",
    "3. Remove trips lasting over two hours.\n",
    "4. Check value ranges of `PULocationID` and `DOLocationID`, and optimize their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6577b11-c4df-4c30-bdd7-23bb1544c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e91e73ea-3cd9-4286-988d-17434bbc0606",
   "metadata": {},
   "source": [
    "## Lazy execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55680d5-d433-4732-a601-2b541d729675",
   "metadata": {},
   "source": [
    "We used the **eager** execution mode before. Now, let's use the **lazy** mode, which is very useful for operating on larger data. Let's read all Parquet files for 2024. Polars allows glob syntax with wildcard `*` to read many files.\n",
    "\n",
    "Let's see what happens for eager `.read_parquet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4a9fd-7619-436f-a337-cec2eac42456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pl.read_parquet(f\"{DATA_DIR}/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87782d08-fd6b-45e8-88a3-9c3f296d1ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T18:21:03.086747Z",
     "iopub.status.busy": "2025-03-08T18:21:03.085980Z",
     "iopub.status.idle": "2025-03-08T18:21:03.142663Z",
     "shell.execute_reply": "2025-03-08T18:21:03.141045Z",
     "shell.execute_reply.started": "2025-03-08T18:21:03.086695Z"
    }
   },
   "source": [
    "We have an error - some files have column `tpep_pickup_datetime` encoded as nanosecond integers, and others as microseconds. This can be fixed with manually casting this column. With eager mode, we would have to:\n",
    "- read list of DataFrames\n",
    "- cast every one\n",
    "- concatenate them\n",
    "\n",
    "However, this is very suboptimal, e.g. DataFrames will be read sequentially, when they could be read in parallel. So let's use the lazy API, with `.scan_parquet()` instead of `.read_parquet()`. It returns a `LazyFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8b29c-bf60-4a33-b2c1-27ad7c095700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet(f\"{DATA_DIR}/1.parquet\")\n",
    "print(type(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5212b-c0f5-46a1-9cc0-7d20cb1c9ac7",
   "metadata": {},
   "source": [
    "No computation has happened yet - we just created a computation graph with a single operation. It's not even optimized yet. Let's build a larger graph, reading all Parquet files and casting the columns appropriately.\n",
    "\n",
    "General type casting uses `.cast()` method. Changing time units like here uses `.dt.cast_time_unit()` instead, as this can be done much faster as a specialized function.\n",
    "\n",
    "We will also call `.explain()` to print the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa2b36-9360-4126-836f-3b22bf1a51ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for month in range(1, 13):\n",
    "    df = pl.scan_parquet(f\"{DATA_DIR}/{month}.parquet\")\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"tpep_pickup_datetime\").dt.cast_time_unit(\"ms\"),\n",
    "        pl.col(\"tpep_dropoff_datetime\").dt.cast_time_unit(\"ms\"),\n",
    "    )\n",
    "    dfs.append(df)\n",
    "\n",
    "df_2024 = pl.concat(dfs)\n",
    "print(df_2024.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97a771-48f7-4463-91be-ead6f60aeca4",
   "metadata": {},
   "source": [
    "Nothing happened yet, and nothing will happen until we call `.collect()` or a lazy output function like e.g. `.sink_parquet()` (that would use streaming mode). Let's do it then. Note that this will load the entire 2024 data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2855fbc-2316-422a-94d4-b9b7b655f32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2024.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27205e-99b1-4d36-9c30-a59cbaf2fe7e",
   "metadata": {},
   "source": [
    "You can all most operations on a LazyFrame, except for the ones that require [knowledge of data schema](https://docs.pola.rs/user-guide/lazy/schemas/#the-lazy-api-must-know-the-schema). It is known only based on materialized data. This is e.g. `.to_dummies()` ([documentation](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.to_dummies.html)), which applies one-hot encoding to a categorical column. It needs to read the data and know the actual possible categories to create columns. Other examples include `.pivot()` and `.columns` attribute.\n",
    "\n",
    "Let's see an example of a lazy query that also heavily filters the data. This will also nicely utilize the ability of Parquet to read only certain columns and rows from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292bfcf2-80d3-4e6e-a14d-86360b6deeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average total taxi fare from the airport among tipping customers\n",
    "(\n",
    "    df_2024.filter(\n",
    "        pl.col(\"Airport_fee\").is_not_null(),\n",
    "        pl.col(\"Airport_fee\") > 0,\n",
    "        pl.col(\"tip_amount\") > 0,\n",
    "    )\n",
    "    .select(\"total_amount\")\n",
    "    .mean()\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f7b9e-a59c-4104-b2a7-f0102601c308",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Compare the speed of the above query in eager and lazy modes.\n",
    "2. Using lazy execution, compare the median taxi are in the first and fourth quarters of 2024.\n",
    "3. Using lazy execution, calculate the median daily values of total amount of taxi fares. Then calculate their rolling sum with a weekly window. Plot the result on a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c84a65-916c-43ee-b4fc-b7cf3008d2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
